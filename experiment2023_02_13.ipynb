{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy, deepcopy\n",
    "from irace2 import irace, dummy_stats_test\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sampling_functions import norm_sample, truncated_poisson, truncated_skellam\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split,StratifiedShuffleSplit,cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import norm, poisson, skellam\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = []\n",
    "ys = []\n",
    "\n",
    "df = pd.read_csv('spect_train.csv')\n",
    "Xs.append(preprocessing.normalize(df.drop(columns=['OVERALL_DIAGNOSIS']).to_numpy()))\n",
    "ys.append(df['OVERALL_DIAGNOSIS'].to_numpy())\n",
    "\n",
    "df = pd.read_csv('spambase.csv')\n",
    "Xs.append(preprocessing.normalize(df.drop(columns=['spam']).to_numpy()))\n",
    "ys.append(df['spam'].to_numpy())\n",
    "\n",
    "df = pd.read_csv('ionosphere_data.csv')\n",
    "Xs.append(preprocessing.normalize(df.drop(columns=['column_ai']).to_numpy()))\n",
    "ys.append(df['column_ai'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 44)\n",
      "(4601, 57)\n",
      "(351, 34)\n",
      "(80,)\n",
      "(4601,)\n",
      "(351,)\n"
     ]
    }
   ],
   "source": [
    "for X in Xs:\n",
    "    print(X.shape)\n",
    "\n",
    "for y in ys:\n",
    "    print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the numeric parameters being configured must be set beforehand\n",
    "models = [LogisticRegression(C=1), \n",
    "    RandomForestClassifier(n_estimators=100,max_depth=5,ccp_alpha=0.0),\n",
    "    SVC(C=1,coef0=0.0),\n",
    "    XGBClassifier(n_estimators=100,max_depth=6,subsample=1)]\n",
    "\n",
    "\n",
    "parameters_dict = {\n",
    "    'LogisticRegression': {'C': lambda loc : norm_sample(loc=loc, scale=1, min= 1e-2),\n",
    "                            'penalty':['l2'],\n",
    "                            'solver':['lbfgs','newton-cg','sag']},\n",
    "    'SVC':{'C':lambda loc : norm_sample(loc=loc, scale=1, min= 1e-2),\n",
    "            'coef0': lambda loc : norm_sample(loc=loc, scale=1, min= 1e-2),\n",
    "            'kernel':['linear','poly','rbf','sigmoid'],\n",
    "            'decision_function_shape':['ovo','ovr']},\n",
    "    'RandomForestClassifier': {'n_estimators': lambda loc: truncated_skellam(loc, mu1=10, mu2=10, min=1), \n",
    "                                'max_depth': lambda loc: truncated_skellam(loc, mu1=1, mu2=1, min=1),\n",
    "                                'max_features':['sqrt', 'log2', None],\n",
    "                                'ccp_alpha':lambda loc : norm_sample(loc=loc, scale=0.1, min= 1e-3)\n",
    "                                },\n",
    "    'XGBClassifier': {'sample_type': ['uniform','weighted'], \n",
    "                        'max_depth': lambda loc: truncated_skellam(loc, mu1=1, mu2=1, min=1),\n",
    "                        'booster':['gbtree','dart'],\n",
    "                        'subsample':lambda loc : norm_sample(loc=loc, scale=0.3, min= 1e-2,max=1)}\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests of hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_tests = [ ss.ttest_rel,\n",
    "                ss.ttest_ind,\n",
    "                ss.mannwhitneyu,\n",
    "                ss.wilcoxon,\n",
    "                dummy_stats_test] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irace parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_id = [0,1,2]\n",
    "train_test_resampling = [10, 30, 100]\n",
    "cv_splits = [10, 30, 100]\n",
    "pop_size = [10, 25, 50]\n",
    "n_gen = [10, 50, 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = list(itertools.product(data_set_id,stat_tests,cv_splits,pop_size,n_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "res = []\n",
    "\n",
    "for n_exp in tqdm(range(n)):\n",
    "    for f in tqdm(factors):\n",
    "        \n",
    "        data_id = f[0]\n",
    "        X = Xs[data_id]\n",
    "        y = ys[data_id]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "        stat_test = f[1]\n",
    "        split = f[2]\n",
    "        p_size = f[3]\n",
    "        stop = f[4]\n",
    "\n",
    "        best_model,best_scores,population,pop_scores = irace(models, \n",
    "                X_train, \n",
    "                y_train, \n",
    "                lambda x: x > stop, \n",
    "                stat_test, \n",
    "                parameters_dict, \n",
    "                p_size, 'f1', cv=split)\n",
    "\n",
    "        best_model.fit(X_train,y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        row = [data_id,stat_test,split,p_size,stop,'cv',type(best_model).__name__,f1_score(y_test,y_pred)]\n",
    "        res.append(row)\n",
    "\n",
    "        best_model,best_scores,population,pop_scores = irace(models, \n",
    "                X_train, \n",
    "                y_train, \n",
    "                lambda x: x > f[4], \n",
    "                stat_test, \n",
    "                parameters_dict, \n",
    "                p_size, 'f1', r=split)\n",
    "\n",
    "        best_model.fit(X_train,y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        row = [data_id,stat_test,split,p_size,stop,'train_test',type(best_model).__name__,f1_score(y_test,y_pred)]\n",
    "        res.append(row)\n",
    "\n",
    "        with open('my_data.pkl', 'wb') as f:\n",
    "            pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_data.pkl', 'rb') as f:\n",
    "    # load the persisted Python object from the file\n",
    "    my_data = pickle.load(f)\n",
    "\n",
    "print(my_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "738f0abc388fde32d91a23dc48b434080bb97b14dd548d1e86272d1e373043ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
